{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[14 数值稳定性 + 模型初始化和激活函数【动手学深度学习v2】](https://www.bilibili.com/video/BV1u64y1i75a?spm_id_from=333.999.0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的梯度\n",
    "- 梯度输出 = 一层输入*二层输出*。。。。。损失函数\n",
    "- $$y = loss * f_d* ... f_1(x)$$\n",
    "- 那么梯度变化等于每层的梯度变化的乘积\n",
    "- $$\\frac{\\partial \\iota}{\\partial W_t} = \\frac{\\partial \\iota}{\\partial h^d}\\frac{\\partial h^d}{\\partial h^{d-1}} ...\\frac{\\partial h^{t+1}}{\\partial h^t}\\frac{\\partial h^t}{\\partial W^t}$$\n",
    "- 一共有d-t次矩阵乘法\n",
    "- 容易导致：一、梯度爆炸 二、梯度消失（就是容易过大或者过小）也就是会带来数值的不稳定性\n",
    "- 公式推导见pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度爆炸问题\n",
    "- 值超出值域\n",
    "  - 对于16位浮点数尤为严重（数值区间为6e-5至6e4）\n",
    "- 对学习率敏感\n",
    "  - 学习率过大->大参数值->更大的梯度\n",
    "  - 学习率过小->训练无法进展\n",
    "  - 我们可能需要在训练过程中不断调整学习率 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 假设\n",
    "  - $w^t_{i,j}$为初始权重，那么这里先要求权重能做到$E[w^t_{i,j}]=0,Var[w^t_{i,j}]=\\gamma$\n",
    "  - $h^{t-1}_i与w^t_{i,j}$相互独立\n",
    "  - 这里首先记得公式$E[\\sum x_iy_j]=\\sum E[x]E[y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先我们看正向方差 \n",
    "  - 这里先不考虑激活函数那么有 $h^t=W^th^{t-1}$\n",
    "    - $Var[h_i^t]=E[h_i^t]^2-[Eh^t_i]^2=E[(\\sum w_{i,j}^th^{t-1}_j)^2]$\n",
    "    - 方差的期望=平方的期望-期望的平方，后者为0\n",
    "    - $=E[\\sum (w^t_{i,j})^2(h^{t-1}_j)^2+\\sum_{j\\neq k} w^t_{i,j}w^t_{i,k}h^{t-1}_jh^{t-1}_k]$\n",
    "    - 同样此项的后半部分同样拆开为多个期望相乘，为0舍去\n",
    "    - $= \\sum_j E[(w^t_{i,j})^2]E[(h^{t-1}_j)^2]$\n",
    "    - 期望为0的情况下，平方的期望就是方差\n",
    "    - $= \\sum_j Var[w^t_{i,j}]Var[h^{t-1}_j]$\n",
    "    - $= n_{t-1}\\gamma_t Var[h^{t-1}_j]$\n",
    "    - $h^{t-1}_j$如果要求正向传播的方差不变，即$Var[h_i^t]=Var[h^{t-1}_j]$，那么则有$n_{t-1}\\gamma_t=1$\n",
    "    - 注：这里n为对应层数的维数，即该层的神经元数量，$\\gamma$则为我们需要设置的方差值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 接下来是反向传播了\n",
    "  - 正向传播是传递函数本身，反向传播传递的是梯度\n",
    "  - 反向传播求的是求损失函数的梯度，实际计算采用链式法则来实现\n",
    "  - 这里只考虑单层的情况，所以不考虑链式法则的影响\n",
    "  - 同样对于单层的感知机$h^t=W^th^{t-1}$\n",
    "  - 满足：$\\frac{\\partial \\iota}{\\partial h^{t-1}} = \\frac{\\partial \\iota}{\\partial h^{t}}W^t$ ; $E[\\frac{\\partial \\iota}{\\partial h^{t-1}}]=0$\n",
    "  - 通过采用与之前正向传播同样的方式，可以计算出\n",
    "  - $Var[\\frac{\\partial \\iota}{\\partial h^{t-1}}]=n_t \\gamma_t Var[\\frac{\\partial \\iota}{\\partial h^{t}}]$\n",
    "  - 那么可以得出$n_t \\gamma_t=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xavier初始化\n",
    "  - 实际中$n_t \\gamma_t=1 ; n_{t-1} \\gamma_t=1$难以同时满足，因为这意味着每一层的维数，即神经元的数量保持不变\n",
    "  - 既然无法同时满足，那么就采取折中方案：$\\gamma_t (n_{t-1}+n_t)/2 =1$\n",
    "  - 可以采用两种方式进行权重分配\n",
    "    - 1、正态分布，$N(0,\\sqrt \\frac{2}{n_{t-1}+n_t})$\n",
    "    - 2、均匀分布，$U(-\\sqrt \\frac{6}{n_{t-1}+n_t},\\sqrt \\frac{6}{n_{t-1}+n_t})$\n",
    "    - 两种分布方差期望均相等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果考虑激活函数，会如何变化？\n",
    "- 如果激活函数为线性函数\n",
    "  - $\\sigma (x) = \\alpha x + \\beta$\n",
    "  - 有$Var[h^t_i] = \\alpha^2Var[W^th^{t-1}_i]$\n",
    "  - 反向亦是如此\n",
    "  - 这里可以推导出$\\alpha = 1 ; \\beta = 0$\n",
    "  - 那么理论上$\\sigma (x) = x$是最好的。。。很无语的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对激活函数采用泰勒展开\n",
    "  - $sigmoid(x) = \\frac {1}{2}+\\frac {x}{4}-\\frac {x^3}{48}$\n",
    "  - $tanh(x) = x - \\frac {x^3}{3}$\n",
    "  - relu(x) = x for  x ≥0\n",
    "  - 这里可看到，tanh和relu近似接近于f(x) = x\n",
    "  - 或者将4sigmoid(x)-2作为激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们得出了什么结论？\n",
    "  - 一、每一层的权重分配可以遵循$N(0,\\sqrt \\frac{2}{n_{t-1}+n_t})$分布\n",
    "  - 二、尽量选择relu函数作为激活函数，皮实耐用"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
